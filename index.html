<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large Language Models</title>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --primary: #667eea;
            --primary-dark: #5568d3;
            --secondary: #764ba2;
            --accent: #f093fb;
            --accent-light: #ffd89b;
            --text: #1a1a2e;
            --text-light: #666;
            --bg: #f8f9fa;
            --white: #ffffff;
            --border: #e8e8f0;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            color: var(--text);
            background-color: var(--bg);
            overflow-x: hidden;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, var(--primary) 0%, var(--secondary) 50%, var(--accent) 100%);
            color: var(--white);
            padding: 6rem 2rem;
            text-align: center;
            position: relative;
            overflow: hidden;
        }

        header::before {
            content: '';
            position: absolute;
            top: -50%;
            right: -10%;
            width: 600px;
            height: 600px;
            background: rgba(255, 255, 255, 0.05);
            border-radius: 50%;
            z-index: 0;
        }

        header::after {
            content: '';
            position: absolute;
            bottom: -30%;
            left: -5%;
            width: 400px;
            height: 400px;
            background: rgba(255, 255, 255, 0.03);
            border-radius: 50%;
            z-index: 0;
        }

        header > * {
            position: relative;
            z-index: 1;
        }

        header h1 {
            font-size: 3.5rem;
            font-weight: 700;
            margin-bottom: 1rem;
            letter-spacing: -1px;
            animation: fadeInDown 0.8s ease-out;
        }

        header .authors {
            font-size: 1.2rem;
            margin-bottom: 0.5rem;
            opacity: 0.95;
            font-weight: 500;
            animation: fadeInUp 0.8s ease-out 0.1s both;
        }

        header .institution {
            font-size: 1rem;
            opacity: 0.85;
            margin-bottom: 2rem;
            animation: fadeInUp 0.8s ease-out 0.2s both;
        }

        header .paper-buttons {
            display: flex;
            gap: 1rem;
            justify-content: center;
            flex-wrap: wrap;
            animation: fadeInUp 0.8s ease-out 0.3s both;
        }

        header .paper-link {
            background: rgba(255, 255, 255, 0.2);
            border: 2px solid var(--white);
            color: var(--white);
            padding: 0.9rem 2rem;
            text-decoration: none;
            border-radius: 50px;
            transition: all 0.3s ease;
            font-weight: 600;
            font-size: 1rem;
            backdrop-filter: blur(10px);
        }

        header .paper-link:hover {
            background: var(--white);
            color: var(--primary);
            transform: translateY(-3px);
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.2);
        }

        /* Navigation */
        nav {
            background-color: var(--white);
            padding: 1.2rem 2rem;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
            position: sticky;
            top: 0;
            z-index: 100;
            backdrop-filter: blur(10px);
            background-color: rgba(255, 255, 255, 0.95);
        }

        nav ul {
            list-style: none;
            display: flex;
            gap: 2.5rem;
            max-width: 1200px;
            margin: 0 auto;
            flex-wrap: wrap;
            justify-content: center;
        }

        nav a {
            text-decoration: none;
            color: var(--text);
            font-weight: 600;
            font-size: 0.95rem;
            position: relative;
            transition: color 0.3s;
        }

        nav a::after {
            content: '';
            position: absolute;
            bottom: -5px;
            left: 0;
            width: 0;
            height: 2px;
            background: linear-gradient(90deg, var(--primary), var(--accent));
            transition: width 0.3s ease;
        }

        nav a:hover {
            color: var(--primary);
        }

        nav a:hover::after {
            width: 100%;
        }

        /* Container */
        .container {
            max-width: 950px;
            margin: 0 auto;
            padding: 0 2rem;
        }

        /* Sections */
        section {
            background: var(--white);
            margin: 3rem 0;
            padding: 4rem 3rem;
            border-radius: 12px;
            box-shadow: 0 4px 30px rgba(0, 0, 0, 0.08);
            border: 1px solid var(--border);
            transition: all 0.3s ease;
            animation: fadeInUp 0.6s ease-out;
        }

        section:hover {
            box-shadow: 0 12px 40px rgba(102, 126, 234, 0.12);
            transform: translateY(-2px);
        }

        #abstract {
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.05) 0%, rgba(118, 75, 162, 0.05) 100%);
            border: 2px solid var(--border);
        }

        h2 {
            color: var(--primary);
            font-size: 2.2rem;
            margin-bottom: 1.5rem;
            font-weight: 700;
            display: flex;
            align-items: center;
            gap: 1rem;
        }

        h2::before {
            content: '';
            width: 5px;
            height: 2.2rem;
            background: linear-gradient(180deg, var(--primary), var(--accent));
            border-radius: 3px;
        }

        h3 {
            color: var(--secondary);
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
        }

        h4 {
            color: var(--primary);
            font-size: 1.1rem;
            font-weight: 600;
            margin-top: 1rem;
            margin-bottom: 0.5rem;
        }

        p {
            margin-bottom: 1.2rem;
            color: var(--text-light);
            line-height: 1.8;
        }

        ul {
            margin-bottom: 1.2rem;
            color: var(--text-light);
            line-height: 1.8;
        }

        /* Highlights */
        .highlights {
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.08) 0%, rgba(240, 147, 251, 0.08) 100%);
            padding: 2rem;
            border-left: 5px solid var(--primary);
            border-radius: 8px;
            margin: 2rem 0;
        }

        .highlights ul {
            list-style: none;
            padding-left: 0;
        }

        .highlights li {
            margin-bottom: 1rem;
            padding-left: 2rem;
            position: relative;
            color: var(--text-light);
            font-weight: 500;
        }

        .highlights li:before {
            content: "‚úì";
            position: absolute;
            left: 0;
            color: var(--primary);
            font-weight: bold;
            font-size: 1.2rem;
        }

        /* Overview Cards */
        .overview-cards {
            display: flex;
            gap: 2rem;
            margin: 3rem 0;
            justify-content: space-between;
        }

        .overview-card {
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.1) 0%, rgba(240, 147, 251, 0.1) 100%);
            padding: 2.5rem;
            border-radius: 12px;
            border: 2px solid var(--border);
            text-align: center;
            transition: all 0.3s ease;
            cursor: pointer;
            text-decoration: none;
            display: block;
            color: inherit;
        }

        .overview-card:hover {
            transform: translateY(-8px);
            box-shadow: 0 15px 40px rgba(102, 126, 234, 0.2);
            border-color: var(--primary);
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.15) 0%, rgba(240, 147, 251, 0.15) 100%);
        }

        .overview-card h3 {
            color: var(--primary);
            font-size: 1.5rem;
            margin: 0;
            margin-bottom: 1rem;
            margin-top: 0;
        }

        .overview-card p {
            color: var(--text-light);
            margin: 0;
            line-height: 1.6;
        }

        /* Results Grid */
        .results-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 2rem;
            margin: 2rem 0;
        }

        .result-card {
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.05) 0%, rgba(240, 147, 251, 0.05) 100%);
            padding: 2rem;
            border-radius: 10px;
            border: 2px solid var(--border);
            transition: all 0.3s ease;
        }

        .result-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 25px rgba(102, 126, 234, 0.15);
            border-color: var(--primary);
        }

        .result-card h4 {
            color: var(--primary);
            margin-bottom: 0.75rem;
            font-size: 1.2rem;
        }

        .result-card p {
            font-size: 0.95rem;
            color: var(--text-light);
        }

        /* Figures */
        figure {
            margin: 2.5rem 0;
            text-align: center;
        }

        figure img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.12);
            transition: all 0.3s ease;
        }

        figure img:hover {
            transform: scale(1.02);
            box-shadow: 0 12px 35px rgba(102, 126, 234, 0.2);
        }

        figcaption {
            margin-top: 1rem;
            font-size: 0.95rem;
            color: var(--text-light);
            font-style: italic;
        }

        /* Two-column figures */
        .figures-container {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 2rem;
            margin: 2.5rem 0;
        }

        .figures-container figure {
            margin: 0;
        }

        .figures-container.small-large {
            grid-template-columns: 0.85fr 1.15fr;
        }

        .figures-container.small-large2 {
            grid-template-columns: 0.75fr 1.25fr;
        }

        .figures-container.large-small {
            grid-template-columns: 1.07fr 0.93fr;
        }


        /* Inline figure */
        .figure-inline {
            float: left;
            width: 45%;
            margin: 0 2rem 1.5rem 0;
        }

        .figure-inline img {
            max-width: 100%;
            height: auto;
        }

        /* Video Transcription Section */
        .video-transcription-container {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2rem;
            margin: 2.5rem 0;
            align-items: start;
        }

        .video-player {
            position: sticky;
            top: 100px;
        }

        .video-player video {
            width: 100%;
            border-radius: 8px;
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
        }

        .video-selector {
            margin-top: 1rem;
            display: flex;
            gap: 0.5rem;
            flex-wrap: wrap;
        }

        .video-btn {
            padding: 0.5rem 1rem;
            background: var(--bg);
            border: 2px solid var(--border);
            border-radius: 6px;
            cursor: pointer;
            transition: all 0.3s ease;
            font-weight: 600;
            color: var(--text);
        }

        .video-btn:hover {
            border-color: var(--primary);
            background: rgba(102, 126, 234, 0.1);
        }

        .video-btn.active {
            background: var(--primary);
            color: white;
            border-color: var(--primary);
        }

        .transcriptions {
            display: flex;
            flex-direction: column;
            gap: 1.5rem;
        }

        .transcription-card {
            background: var(--bg);
            padding: 1.5rem;
            border-radius: 8px;
            border: 2px solid var(--border);
            transition: all 0.3s ease;
        }

        .transcription-card:hover {
            border-color: var(--primary);
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.15);
        }

        .transcription-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1rem;
            padding-bottom: 0.75rem;
            border-bottom: 2px solid var(--border);
        }

        .model-name {
            font-weight: 700;
            color: var(--primary);
            font-size: 1.1rem;
        }

        .wer-badge {
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: white;
            padding: 0.3rem 0.8rem;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
        }

        .transcription-text {
            color: var(--text-light);
            line-height: 1.7;
            font-size: 0.95rem;
        }

        /* Ground Truth Card */
        .ground-truth-card {
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.15) 0%, rgba(118, 75, 162, 0.15) 100%);
            padding: 1.5rem;
            border-radius: 8px;
            border: 3px solid var(--primary);
            margin-bottom: 2rem;
        }

        .ground-truth-card .transcription-header {
            margin-bottom: 1rem;
            padding-bottom: 0.75rem;
            border-bottom: 2px solid var(--primary);
        }

        .ground-truth-card .model-name {
            font-size: 1.2rem;
            color: var(--primary);
        }

        .ground-truth-card .transcription-text {
            font-weight: 500;
            color: var(--text);
        }


        /* Comparison Table */
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 2rem 0;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.08);
        }

        .comparison-table th,
        .comparison-table td {
            padding: 1.2rem;
            text-align: left;
        }

        .comparison-table th {
            background: linear-gradient(135deg, var(--primary) 0%, var(--secondary) 100%);
            color: var(--white);
            font-weight: 600;
        }

        .comparison-table td {
            border-bottom: 1px solid var(--border);
            color: var(--text-light);
        }

        .comparison-table tbody tr:last-child td {
            border-bottom: none;
        }

        .comparison-table tbody tr:hover {
            background-color: rgba(102, 126, 234, 0.05);
        }

        .comparison-table strong {
            color: var(--primary);
        }

        /* Citation Section */
        .citation-box {
            background-color: #f5f5f5;
            padding: 2rem;
            border-radius: 8px;
            border-left: 5px solid var(--primary);
            overflow-x: auto;
            margin: 2rem 0;
        }

        .citation-box pre {
            margin: 0;
            font-family: 'Monaco', 'Courier New', monospace;
            font-size: 0.9rem;
            color: var(--text);
            line-height: 1.6;
        }

        /* Footer */
        footer {
            background: linear-gradient(135deg, var(--text) 0%, #2a2a4e 100%);
            color: var(--white);
            text-align: center;
            padding: 3rem 2rem;
            margin-top: 4rem;
        }

        footer a {
            color: var(--accent-light);
            text-decoration: none;
            transition: color 0.3s;
        }

        footer a:hover {
            color: var(--accent);
        }

        /* Animations */
        @keyframes fadeInDown {
            from {
                opacity: 0;
                transform: translateY(-30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        
        /* Responsive */
        @media (max-width: 1024px) {
            .overview-cards {
                flex-wrap: wrap;
            }

            .overview-card {
                flex: 1 1 calc(50% - 1rem);
                min-width: 200px;
            }

            .figures-container {
                grid-template-columns: 1fr;
            }
        }

        @media (max-width: 768px) {
            header {
                padding: 3rem 1.5rem;
            }

            header h1 {
                font-size: 1.8rem;
                line-height: 1.3;
            }

            header .authors {
                font-size: 1rem;
            }

            header .institution {
                font-size: 0.9rem;
            }

            header .paper-link {
                padding: 0.7rem 1.5rem;
                font-size: 0.9rem;
            }

            nav {
                padding: 1rem 1rem;
            }

            nav ul {
                gap: 0.75rem;
                padding: 0;
            }

            nav a {
                font-size: 0.8rem;
                font-weight: 600;
            }

            .container {
                padding: 0 1rem;
            }

            section {
                padding: 2rem 1.5rem;
                margin: 1.5rem 0;
            }

            h2 {
                font-size: 1.6rem;
            }

            h2::before {
                width: 4px;
                height: 1.6rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .overview-cards {
                grid-template-columns: 1fr;
                gap: 1rem;
            }

            .overview-card {
                padding: 1.5rem;
            }

            .overview-card h3 {
                font-size: 1.2rem;
            }

            .results-grid {
                grid-template-columns: 1fr;
                gap: 1rem;
            }

            .result-card {
                padding: 1.5rem;
            }

            .comparison-table {
                font-size: 0.85rem;
            }

            .comparison-table th,
            .comparison-table td {
                padding: 0.6rem 0.4rem;
            }

            .video-transcription-container {
                grid-template-columns: 1fr;
                gap: 1.5rem;
            }

            .video-player {
                position: relative;
                top: 0;
            }

            .video-btn {
                font-size: 0.85rem;
                padding: 0.5rem 0.8rem;
            }

            .transcription-card {
                padding: 1rem;
            }

            .ground-truth-card {
                padding: 1rem;
            }

            .model-name {
                font-size: 1rem;
            }

            .wer-badge {
                font-size: 0.75rem;
                padding: 0.25rem 0.6rem;
            }

            .transcription-text {
                font-size: 0.9rem;
            }

            .figures-gallery {
                grid-template-columns: 1fr;
                gap: 1rem;
            }

            .modal-content {
                margin: 1rem;
                max-height: 85vh;
            }

            .modal-body {
                padding: 1.5rem;
            }

            .modal-body h3 {
                font-size: 1.4rem;
            }

            figure img {
                border-radius: 6px;
            }

            .citation-box {
                padding: 1rem;
                font-size: 0.85rem;
            }

            .citation-box pre {
                font-size: 0.8rem;
            }

            footer {
                padding: 2rem 1rem;
                font-size: 0.9rem;
            }
        }

        @media (max-width: 480px) {
            header h1 {
                font-size: 1.5rem;
            }

            header .paper-buttons {
                flex-direction: column;
                width: 100%;
            }

            header .paper-link {
                width: 100%;
                text-align: center;
            }

            nav ul {
                gap: 0.5rem;
                justify-content: space-around;
            }

            nav a {
                font-size: 0.75rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .overview-card {
                padding: 1.25rem;
            }

            .comparison-table {
                font-size: 0.75rem;
            }

            .comparison-table th,
            .comparison-table td {
                padding: 0.5rem 0.3rem;
            }

            .video-selector {
                flex-direction: column;
            }

            .video-btn {
                width: 100%;
            }
        }
        
        
    </style>
</head>
<body>
    <header>
        <h1 style="font-family:'Comic Sans MS';color:#343434">Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large Language Models</h1>
        <div class="authors">Umberto Cappellazzo<sup style="color:red;">&spades;</sup>, Xubo Liu<sup style="color:red;">&clubs;</sup>, Pingchuan Ma<sup style="color:red;">&spades;</sup>, Stavros Petridis<sup style="color:red;">&spades;,&diams;</sup>, Maja Pantic<sup style="color:red;">&spades;,&diams;</sup></div>
        <div class="institution">
            <sup style="color:red;">&spades;</sup><span style="font-family:'Comic Sans MS';color:#343434;"> Imperial College London</span> &nbsp; <sup style="color:red;">&clubs;</sup><span style="font-family:'Comic Sans MS';color:#343434"> University of Surrey</span> &nbsp; <sup style="color:red;">&diams;</sup><span style="font-family:'Comic Sans MS';color:#343434"> NatWest AI Research</span>
        </div>
        <div class="paper-buttons">
            <a href="https://arxiv.org/abs/2511.07253" class="paper-link" style="font-family:'Comic Sans MS'">üìÑ PDF Paper</a>
            <a href="https://github.com/umbertocappellazzo/Omni-AVSR" class="paper-link" style="font-family:'Comic Sans MS'"><i class="fab fa-github"></i> Code & Weights</a>
        </div>
    </header>

    <nav>
        <ul>
            <li><a href="#abstract" style="font-family:'Comic Sans MS'">Abstract</a></li>
            <li><a href="#introduction" style="font-family:'Comic Sans MS'">Overview</a></li>
            <li><a href="#method" style="font-family:'Comic Sans MS'">Omni-AVSR Details</a></li>
            <li><a href="#results" style="font-family:'Comic Sans MS'">Main Results</a></li>
            <li><a href="#qualitative results" style="font-family:'Comic Sans MS'">Qualitative Results</a></li>
            <li><a href="#ablations" style="font-family:'Comic Sans MS'">Ablation Studies</a></li>
            <li><a href="#conclusion" style="font-family:'Comic Sans MS'">Conclusion</a></li>
            <li><a href="#citation" style="font-family:'Comic Sans MS'">Citation</a></li>
        </ul>
    </nav>

    <div class="container">
        <!-- Abstract Section -->
        <section id="abstract">
            <h2 style="font-family:'Comic Sans MS'">Abstract</h2>
            <p>
                Large language models (LLMs) have recently achieved impressive results in speech recognition across multiple modalities, including Auditory Speech Recognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech Recognition (AVSR). Despite this progress, current LLM-based approaches typically address each task independently, training separate models that raise computational and deployment resource use while missing potential cross-task synergies. They also rely on fixed-rate token compression, which restricts flexibility in balancing accuracy with efficiency. These limitations highlight the need for a unified framework that can support ASR, VSR, and AVSR while enabling elastic inference. To this end, we present Omni-AVSR, a unified audio-visual LLM that combines efficient multi-granularity training with parameter-efficient adaptation. Specifically, we adapt the matryoshka representation learning paradigm to efficiently train across multiple audio and visual granularities, reducing its inherent training resource use. Furthermore, we explore three LoRA-based strategies for adapting the backbone LLM, balancing shared and task-specific specialization. Experiments on LRS2 and LRS3 show that Omni-AVSR achieves comparable or superior accuracy to state-of-the-art baselines while training a single model at substantially lower training and deployment resource use. The model also remains robust under acoustic noise, and we analyze its scaling behavior as LLM size increases, providing insights into the trade-off between performance and efficiency.
            </p>
            <!--
            <div class="highlights">
                <ul>
                    <li>Key contribution or finding 1</li>
                    <li>Key contribution or finding 2</li>
                    <li>Key contribution or finding 3</li>
                </ul>
            </div>
        -->
        </section>

        <!-- Introduction Section -->
        <section id="introduction">
            <h2 style="font-family:'Comic Sans MS'">Overview</h2>

            <div class="overview-cards">
                <a href="#method" class="overview-card">
                    <h3 style="font-family:'Comic Sans MS'">OMNI-AVSR Details</h3>
                    <p>Learn about Omni-AVSR key components: efficient matryoshka traning and LoRA-based finetuning.</p>
                </a>
                <a href="#results" class="overview-card">
                    <h3 style="font-family:'Comic Sans MS'">Main Results</h3>
                    <p>Omni-AVSR results against state-of-the-art methods on LRS2 and LRS3 datasets.</p>
                </a>
                <a href="#ablations" class="overview-card">
                    <h3 style="font-family:'Comic Sans MS'">Ablation Studies</h3>
                    <p>Explore Omni-AVSR scaling behavior or the importance of each task in the training paradigm.</p>
                </a>
            </div>

        </section>

        <!-- Method Section -->
        <section id="method">
            <h2 style="font-family:'Comic Sans MS'">Omni-AVSR Details</h2>
            <p>
                The goal of Omni-AVSR is to train a <b>single unified</b> LLM-based model capable of performing ASR, VSR, and AVSR. At the same time, it enables <i>flexible control of audio-visual granularity at inference</i> according to resource constraints. In this way, Omni-AVSR supports multiple modalities and granularities within a single set of weights, while reducing training and deployment costs and achieving performance on par with, or even surpassing, state-of-the-art models trained independently for specific tasks or granularities. <br>

                Following prior audio-visual LLMs (e.g., <a href="https://arxiv.org/abs/2409.12319" target="_blank">Llama-AVSR</a>, <a href="https://arxiv.org/abs/2505.14336" target="_blank">Llama-SMoP</a>), Omni-AVSR comprises <b><mark style="background-color: #9ACD32;">pre-trained audio and video encoders</mark></b>,<b> <mark style="background-color: #B0C4DE;">projection layers</mark></b>, and an <b><mark style="background-color: #DCDCDC;">LLM backbone</mark></b> (see <b>Figure 1a</b>). In the next sections, we detail how Omni-AVSR is endowed with <b>1)</b> explicit control over audio-visual granularities during inference and <b>2)</b> the ability to jointly support ASR, VSR, and AVSR within a single model.
                
            </p>

            <figure>
                <img src="./images/Omni-AVSR_overview.png" alt="Method Overview">
                <figcaption><b>Figure 1</b>: Overview of <b>(a)</b> the proposed Omni-AVSR model and <b>(b)</b> its Omni-LoRA variants.</figcaption>
            </figure>

            <h3 style="font-family:'Comic Sans MS'">Multi-Granularity via Efficient Matryoshka Training</h3>
            <p>
                Omni-AVSR introduces a multi-granularity training scheme that allows the model to flexibly trade off efficiency and accuracy during inference. Instead of relying on fixed audio-visual compression rates, Omni-AVSR builds on the <b>Matryoshka Representation Learning</b> (MRL) principle to support inference at multiple token granularities within a single unified model. <br> <br>

                Given an input audio waveform \( \mathbf{a} \) and its corresponding lip-movement video \( \mathbf{v} \), the pre-trained encoders produce token sequences \( \mathbf{Z}^a \) and \( \mathbf{Z}^v \), respectively. During training, token sequences at varying granularities are generated by applying \( C_A \) audio compression rates {\(a_1, a_2,\cdots,a_{C_A} \)} and \( C_V \) video rates {\(v_1, v_2,\cdots,v_{C_V} \)}. <br>

                Na√Øvely combining these rates would require \( C_A \) LLM forward/backward passes per batch for <i>ASR</i>, \( C_V \) for <i>VSR</i>, and \( C_A \cdot C_V \) for <i>AVSR</i>. <i>This would lead to prohibitive computational overhead and potential interference among multiple objectives</i>. <br> <br>

                To overcome this limitation, we introduce a <b>key modification</b>: during training, we randomly select one audio rate \( a_i \) and one video rate \( v_j \) at each iteration, yielding compressed sequences \( \mathbf{Z}^{a_i} \) and \( \mathbf{Z}^{v_j} \). This reduces the number of forward/backward LLM passes to only three, one per task, instead of \( C_A + C_V + C_A \cdot C_V \). <br>

                These compressed sequences are then passed through modality-specific projection layers and concatenated with task-specific text tokens \( X_t^{\text{P}} \), where \( t \in \{\mathsf{ASR}, \mathsf{VSR}, \mathsf{AVSR}\} \) and \(X^\text{P}_t\) encodes both the task prompt and the transcription. Therefore, we obtain: \(\mathbf{Z}_{\mathsf{ASR}} = [\mathbf{Z}^{a_i}, X^\text{P}_{\mathsf{ASR}}]\), \(\mathbf{Z}_{\mathsf{VSR}} = [\mathbf{Z}^{v_j}, X^\text{P}_{\mathsf{VSR}}]\), and \(\mathbf{Z}_{\mathsf{AVSR}} = [\mathbf{Z}^{a_i}, \mathbf{Z}^{v_j} , X^\text{P}_{\mathsf{AVSR}}]\). <i>This strategy preserves the flexibility of MRL at inference while substantially reducing its training cost</i>.
            </p>

            <h3 style="font-family:'Comic Sans MS'">Joint ASR-VSR-AVSR Training Formulation</h3>
            <p>
                Omni-AVSR is trained by averaging the auto-regressive next token prediction loss for each task for each input data. For each task-specific sequence \(\mathbf{Z}_t\), the probability of the target \(\mathbf{Y}\) is computed by \(p(\mathbf{Y}|\mathbf{Z}_t) = \prod_{s=1}^{S}p_\theta(y_s|\mathbf{Z}_t, y_{< s})\), and the corresponding loss is defined as \(\mathcal{L}_t = - \log p(\mathbf{Y}|\mathbf{Z}_t)\), where \(y_{< s}\) is the generated output sequence up to token \(s-1\), \(\theta\) is the trainable parameters, and \(t \in \{\mathsf{ASR}, \mathsf{VSR}, \mathsf{AVSR}\}\). Overall, the final objective we train on is:

                $$\mathcal{L}_{\text{OMNI}} = \lambda_{\mathsf{ASR}}\mathcal{L}_{\mathsf{ASR}} + \lambda_{\mathsf{VSR}}\mathcal{L}_{\mathsf{VSR}} + \lambda_{\mathsf{AVSR}}\mathcal{L}_{\mathsf{AVSR}},$$
                where \(\lambda_{\mathsf{ASR}}\), \(\lambda_{\mathsf{VSR}}\), \(\lambda_{\mathsf{AVSR}}\) are task-specific weights.
            </p>

            <h3 style="font-family:'Comic Sans MS'">Efficient LLM Adaptation via Omni-LoRA</h3>
            <p>
                In Omni-AVSR, the pre-trained LLM is kept frozen while low-rank LoRA modules are employed to parameter-efficiently fine-tune it. Given our multi-task setting, we explore three configurations: <b>1)</b> Omni-LoRA-<b>S</b>, <b>2)</b> Omni-LoRA-<b>T</b>, and <b>3)</b> Omni-LoRA-<b>ST</b>, illustrated in <b>Figure 1b</b>. <br>

                <ul>
                    <li>The Omni-LoRA-<b>S</b> variant employs a <i>single</i> <b>S</b>hared LoRA module to adapt the query and value projection matrices of each LLM self- attention layer across ASR, VSR, and AVSR tasks.</li>
                    <li>The Omni-LoRA-<b>T</b> variant instead defines <i>separate</i> <b>T</b>ask-specific LoRA modules, each specialized to one specific task.</li>
                    <li>Omni-LoRA-<b>ST</b> combines both <b>S</b>hared and <b>T</b>ask-specific LoRA modules, resulting in a hybrid approach of the previous variants.</li>
                </ul>

                <p>
                During <b>training</b>, Omni-LoRA-T and Omni-LoRA-ST activate <i>all</i> task-specific modules. At <b>inference</b>, however, <i>only</i> the module corresponding to the selected task is used, ensuring efficiency.
                </p>

            </p>
        </section>

        <!-- Main Results Section -->
        <section id="results">
            <h2 style="font-family:'Comic Sans MS'">Main Results</h2>
            <p>
                We conduct experiments on <a href="https://arxiv.org/abs/1611.05358" target="_blank">LRS2</a> and <a href="https://arxiv.org/abs/1809.00496" target="_blank">LRS3</a> datasets. For a detailed description of these datasets, the pre-processing, the traning/inference details and more, please take a look at our paper.
            </p>


            <div class="figures-container small-large">
                <figure>
                    <img src="./images/Main_table.png" alt="Method Figure 1">
                    <figcaption><b>Table 1</b>: ASR, VSR, AVSR results in terms of WER (%) across different audio and video compression rates.</figcaption>
                </figure>
                <figure>
                    <img src="./images/Unified_comparison.png" alt="Method Figure 2">
                    <figcaption><b>Table 2</b>: Comparison with state-of-the-art methods using a single model for ASR, VSR, and AVSR on LRS3.</figcaption>
                </figure>
            </div>

            <h3 style="font-family:'Comic Sans MS'">Main ASR/VSR/AVSR Results on LRS2/LRS3</h3>

            <p>
                <b>Table 1</b> reports the ASR/VSR/AVSR results of our three Omni-AVSR variants on LRS2 and LRS3. On LRS2, the task- specific variant Omni-AVSR-T achieves the best performance, while on LRS3 all three variants yield comparable results. Compared with the baselines, we observe the following: <b>(1)</b> all Omni-AVSR variants consistently outperform Llama-AVSR, which requires a separate model per rate and task; <b>(2)</b> Omni-AVSR-T on LRS2, and all three variants on LRS3, match or surpass Llama-MTSK and Llama-MT; <b>(3)</b> task-wise, Omni-AVSR particularly benefits VSR; and <b>(4)</b> performance trends remain consistent across compression rates.
            </p>

            <h3 style="font-family:'Comic Sans MS'">Computational Cost Analysis</h3>

            <p>
                Beyond delivering strong recognition performance, Omni-AVSR
                also offers significant computational advantages, as summarized in <b>Table 2</b>. <b>(1)</b> Omni-AVSR requires training only a <i>single</i> model, independent of the number of tasks \(T\) (ASR, VSR, and AVSR in our case, so \(T\) = 3) and the number of audio \(C_A\) and video \(C_V\) compression rates (\(C_A = C_V = 2\) in our setup). <b>(2)</b> In terms of the number of forward/backward passes required over the LLM, Omni-AVSR computes the loss only once per task, as it samples a single audio and video rate at each iteration, thus reducing the requirement to just T passes. Overall, Omni-AVSR <i>requires only a single model and substantially reduces overall training computations compared to all baselines</i>.
            </p>

            <h3 style="font-family:'Comic Sans MS'">Results under Acoustic Noise</h3>
            
            <p>
                To evaluate the robustness of Omni-AVSR under noisy conditions, we inject babble noise at varying SNRs. As shown in <b>Table 3</b>, Omni-AVSR-ST consistently outperforms Llama-AVSR and Llama-MTSK, and remains competitive with Llama-MT across noise levels, often surpassing it at lower SNRs.
            </p>

            <h3 style="font-family:'Comic Sans MS'">Comparison with Other Multi-task Methods</h3>
            
            <p>
                In <b>Table 4</b>, we compare Omni-AVSR-ST with three state-of- the-art methods that train a single model for ASR, VSR, and AVSR: <a href="https://arxiv.org/abs/2207.07036" target="_blank">u-HuBERT</a>, <a href="https://www.mdpi.com/2079-9292/14/12/2310" target="_blank">MultiAVSR</a>, and <a href="https://arxiv.org/abs/2411.02256" target="_blank">USR</a>. At the (4,2) compression setting, Omni-AVSR-ST achieves the best performance across all tasks while requiring significantly fewer parameters and surpassing u-HuBERT, despite the latter being trained on 1759 hours of data.
            </p>


            <div class="figures-container small-large2">
                <figure>
                    <img src="./images/Noise.png" alt="Method Figure 1">
                    <figcaption><b>Table 3</b>: AVSR results on LRS3 across acoustic noise conditions.</figcaption>
                </figure>
                <figure>
                    <img src="./images/Computation_cost.png" alt="Method Figure 2">
                    <figcaption><b>Table 4</b>: Computational cost analysis. Here, <b>T</b> denotes the number of tasks, while \(\mathbf{C_A}/\mathbf{C_V}\) denotes the number of audio/video rates.</figcaption>
                </figure>
            </div>

            
        </section>


        <!-- Qualitative Results Section -->
        <section id="qualitative results">
            <h2 style="font-family:'Comic Sans MS'">Qualitative Results</h2>
            <p>
                Below we include some clip videos from the LRS3 test set and the corresponding generated transcriptions using Omni-AVSR-ST using audio-only, video-only, or audio-visual modalities at different compression rates. The *WER* displayed is the one obtained by each model across the 5 videos.
            </p>

            <div class="video-transcription-container">
                <!-- Left: Video Player -->
                <div class="video-player">
                    <video id="videoPlayer" controls>
                        <source src="videos/dYNc3P4jSt4_00004_h264.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="video-selector">
                        <button class="video-btn active" onclick="switchVideo('videos/dYNc3P4jSt4_00004_h264.mp4', 0)">Video 1</button>
                        <button class="video-btn" onclick="switchVideo('videos/Sa27SUR0Mlo_00013_h264.mp4', 1)">Video 2</button>
                        <button class="video-btn" onclick="switchVideo('videos/uqS6T6TAu74_00001_h264.mp4', 2)">Video 3</button>
                        <button class="video-btn" onclick="switchVideo('videos/YD5PFdghryc_00001_h264.mp4', 3)">Video 4</button>
                        <button class="video-btn" onclick="switchVideo('videos/ZJNESMhIxQ0_00001_h264.mp4', 4)">Video 5</button>
                    </div>
                </div>

                <!-- Right: Transcriptions -->
                <div class="transcriptions" id="transcriptionsContainer">
                    <!-- Ground Truth -->
                    <div class="ground-truth-card">
                        <div class="transcription-header">
                            <span class="model-name">üìù Ground Truth</span>
                        </div>
                        <div class="transcription-text" id="ground-truth-text">
                            she never had to duck and cover under her desk at school
                        </div>
                    </div>

                <div class="transcriptions" id="transcriptionsContainer">
                    <!-- Model 1 -->
                    <div class="transcription-card">
                        <div class="transcription-header">
                            <span class="model-name">Audio-only, Rate: 4</span>
                            <span class="wer-badge">WER: 1.06%</span>
                        </div>
                        <div class="transcription-text" id="model1-text">
                            she never had to duck and cover under her desk at school
                        </div>
                    </div>

                    <!-- Model 2 -->
                    <div class="transcription-card">
                        <div class="transcription-header">
                            <span class="model-name">Audio-only, Rate: 16</span>
                            <span class="wer-badge">WER: 2.13%</span>
                        </div>
                        <div class="transcription-text" id="model2-text">
                            she never had to duck and cover under her desk at school
                        </div>
                    </div>

                    <!-- Model 3 -->
                    <div class="transcription-card">
                        <div class="transcription-header">
                            <span class="model-name">Video-only, Rate: 2</span>
                            <span class="wer-badge">WER: 18.08%</span>
                        </div>
                        <div class="transcription-text" id="model3-text">
                            she never had a steady cover under her desk at school
                        </div>
                    </div>

                    <!-- Model 4 -->
                    <div class="transcription-card">
                        <div class="transcription-header">
                            <span class="model-name">Video-only, Rate: 5</span>
                            <span class="wer-badge">WER: 17.02%</span>
                        </div>
                        <div class="transcription-text" id="model4-text">
                            she never had a steady cover under her desk at school
                        </div>
                    </div>

                    <!-- Your Model (OMNI-AVSR) -->
                    <div class="transcription-card">
                        <div class="transcription-header">
                            <span class="model-name">Audio-visual, A Rate: 4, V Rate: 2</span>
                            <span class="wer-badge">WER: 1.06%</span>
                        </div>
                        <div class="transcription-text" id="model5-text">
                            she never had to duck and cover under her desk at school
                        </div>
                    </div>

                    <div class="transcription-card">
                        <div class="transcription-header">
                            <span class="model-name">Audio-visual, A Rate: 16, V Rate: 5</span>
                            <span class="wer-badge">WER: 1.06%</span>
                        </div>
                        <div class="transcription-text" id="model6-text">
                            she never had to duck and cover under her desk at school
                        </div>
                    </div>

                </div>
            </div>
        </section>

        <!-- Ablation Results Section -->
        <section id="ablations">
            <h2 style="font-family:'Comic Sans MS'">Ablation Studies</h2>
            
            <h3 style="font-family:'Comic Sans MS'">Optimal Task-specific Weights</h3>

            <figure class="figure-inline">
                <img src="./images/Coefficients.png" alt="Description">
                <figcaption><b>Table 5</b>: Ablation on the best values of ASR/VSR/AVSR weights.</figcaption>
            </figure>

            <p>In <b>Table 5</b>, we analyze the impact of varying the loss weight coefficients for each task on the LRS2 dataset. The best performance is given by \(\lambda_{\mathsf{ASR}} = \lambda_{\mathsf{AVSR}} = 1\) and \(\lambda_{\mathsf{VSR}} = 1.5\). Since VSR is the most challenging of the three tasks, assigning it a higher weight leads to improved overall results.</p>

            <h3 style="font-family:'Comic Sans MS'">AVSR Comparison with Sota Methods</h3>
            <p>
                <b>Figure 2</b> presents a comparison of Omni-AVSR-ST with recent state-of-the-art approaches, whose details can be found in our paper. Omni-AVSR-ST (evaluated at audio-video rates of (4,2)) achieves competitive WERs while requiring substantially fewer parameters and training data hours than all baselines, within one consistent framework.
            </p>

            <h3 style="font-family:'Comic Sans MS'">LLM Scaling Trend</h3>
            <p>
                 We study how scaling the LLM impacts performance across ASR, VSR, and AVSR in <b>Figure 3</b>, using models of different sizes from the Llama and Qwen 2.5 families. As shown, performance improves with larger LLMs, with higher gains observed on more challenging tasks (e.g., VSR) or under higher compression (e.g., ASR at rate 16). However, larger models incur greater training computations, memory usage, and slower inference. <i>Overall, LLMs in the 1‚Äì3B parameter range represent a favorable trade-off between accuracy and efficiency</i>.
            </p>


        <div class="figures-container large-small">
                <figure>
                    <img src="./images/bubblechart.png" alt="Method Figure 1">
                    <figcaption><b>Figure 2</b>: Comparison of Omni-AVSR-ST with state-of-the-art AVSR methods in terms of <i>WER</i>, <i>activated parameters</i>, and <i>training data hours</i> on LRS3.</figcaption>
                </figure>
                <figure>
                    <img src="./images/scaling_LLM.png" alt="Method Figure 2">
                    <figcaption><b>Figure 3</b>: <i>Scaling</i> trend of Omni-AVSR-ST when we increase the LLM size on LRS3.</figcaption>
                </figure>
            </div>


        </section>




        <!-- Conclusion Section -->
        <section id="conclusion">
            <h2 style="font-family:'Comic Sans MS'">Conclusion</h2>
            <p>
                In this work, we introduce Omni-AVSR, the first unified audio-visual LLM that jointly supports ASR, VSR, and AVSR while enabling elastic inference under a single set of weights. By combining efficient matryoshka-based multi-granularity training with LoRA adaptation strategies, Omni-AVSR achieves strong performance while reducing training and deployment costs. Experiments on LRS2 and LRS3 show that Omni-AVSR matches or surpasses state-of-the-art baselines, remains robust in noisy conditions, and delivers favorable trade-offs when scaling LLM size. Furthermore, Omni-AVSR provides significant computational savings, requiring only one model and a reduced number of LLM passes during training.
            </p>
            
        </section>

        <!-- Citation Section -->
        <section id="citation">
            <h2 style="font-family:'Comic Sans MS'">Citation</h2>
            <p>If you find this work useful, please cite our paper using the following BibTeX entry:</p>
            <div class="citation-box">
                <pre>@article{Cappellazzo2025Omni-AVSR,
  title={Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large Language Models},
  author={Umberto, Cappellazzo and Xubo, Liu and Pingchuan, Ma and Stavros, Petridis and Maja, Pantic},
  journal={arxiv 2511.07253},
  year={2025},
}</pre>
            </div>
            Should you have any questions or would like to collaborate in advancing LLM-based audio-visual speech recognition or similar topics, feel free to touch base with me at umbertocappellazzo@gmail.com
        </section>
    </div>

    <footer>
        <p>&copy; 2025 Umberto Cappellazzo. | <a href="https://github.com/umbertocappellazzo">GitHub</a> | <a href="https://x.com/Umberto_Senpai">Twitter</a> | <a href="https://www.linkedin.com/in/umberto-cappellazzo-116093150/">LinkedIn</a></p>
    </footer>

    <script>
        // Transcription data for each video and model
        const transcriptionData = {
            0: { // Video 1
                groundTruth: "she never had to duck and cover under her desk at school",
                model1: { text: "she never had to duck and cover under her desk at school", wer: "1.06%" },
                model2: { text: "she never had to duck and cover under her desk at school", wer: "2.13%" },
                model3: { text: "she never had a steady cover under her desk at school", wer: "18.08%" },
                model4: { text: " she never had a steady cover under her desk at school", wer: "17.02%" },
                model5: { text: "she never had to duck and cover under her desk at school", wer: "1.06%" },
                model6: { text: "she never had to duck and cover under her desk at school", wer: "1.06%" }
            },
            1: { // Video 2
                groundTruth: "there's going to be a new system based on donated package tracking technology from the logistics company that i work for",
                model1: { text: "there's going to be a new system based on donated package tracking technology from the logistics company that i work for", wer: "1.06%" },
                model2: { text: "there's going to be a new system based on donated packaged tracking technology from the logistics company that i work for", wer: "2.13%" },
                model3: { text: "there's going to be a new system based on donating back and tracking technology from the lucasgas company that i work for", wer: "18.08%" },
                model4: { text: "there's going to be a new system based on donating backtracking technology from the lucy's company that i work for", wer: "17.02%" },
                model5: { text: "there's going to be a new system based on donated package tracking technology from the logistics company that i work for", wer: "1.06%" },
                model6: { text: "there's going to be a new system based on donated package tracking technology from the logistics company that i work for", wer: "1.06%" }
            },
            2: { // Video 3
                groundTruth: "i became a psychology researcher and i've devoted my work to understanding the human capacity to care for others",
                model1: { text: "became a psychology researcher and i've devoted my work to understanding the human capacity to care for others", wer: "1.06%" },
                model2: { text: "i became a psychology researcher and i've devoted my work to understanding the human capacity to care for others", wer: "2.13%" },
                model3: { text: "became a psychology researcher and i've devoted my work to understanding the human capacity to care for others", wer: "18.08%" },
                model4: { text: "became a psychology researcher and i've devoted my work to understanding the human capacity to care for others", wer: "17.02%" },
                model5: { text: "became a psychology researcher and i've devoted my work to understanding the human capacity to care for others", wer: "1.06%" },
                model6: { text: "i became a psychology researcher and i've devoted my work to understanding the human capacity to care for others", wer: "1.06%" }
            },
            3: { // Video 4
                groundTruth: "and so many of my early memories involved intricate daydreams where i would walk across borders",
                model1: { text: "and so many of my early memories involved intricate daydreams where i would walk across borders", wer: "1.06%" },
                model2: { text: "and so many of my early memories involved intimate daydreams where i would walk across borders", wer: "2.13%" },
                model3: { text: "and so many of my early memories involve trying to change tangles around how to walk across borders", wer: "18.08%" },
                model4: { text: "and so many of my early memories involve flinging change tangles or how i'd walk across borders", wer: "17.02%" },
                model5: { text: "and so many of my early memories involved intricate daydreams where i would walk across borders", wer: "1.06%" },
                model6: { text: "and so many of my early memories involved intimate daydreams where i would walk across borders", wer: "1.06%" }
            },
            4: { // Video 4
                groundTruth: "it turned out that we were doing a lot of low level drug cases on the streets just around the corner from our office in trenton",
                model1: { text: "it turned out that we were doing a lot of low level drug cases on the streets just around the corner from our office in trenton", wer: "1.06%" },
                model2: { text: "it turned out that we were doing a lot of low level drug cases on the streets just around the corner from our office in trenton", wer: "2.13%" },
                model3: { text: "it turned out that we were doing a lot of low level drug cases on the streets just around the corner from our office in downtown", wer: "18.08%" },
                model4: { text: "it turned out that we were doing a lot of low level drug cases on the streets just around the corner from our office in downtown", wer: "17.02%" },
                model5: { text: "it turned out that we were doing a lot of low level drug cases on the streets just around the corner from our office in trenton", wer: "1.06%" },
                model6: { text: "it turned out that we were doing a lot of low level drug cases on the streets just around the corner from our office in trenton", wer: "1.06%" }
            }
        };

        // Switch video and update transcriptions
        function switchVideo(videoSrc, videoIndex) {
            // Update video source
            const videoPlayer = document.getElementById('videoPlayer');
            videoPlayer.src = videoSrc;
            videoPlayer.load();

            // Update active button
            document.querySelectorAll('.video-btn').forEach((btn, index) => {
                btn.classList.toggle('active', index === videoIndex);
            });

            // Update transcriptions
            const data = transcriptionData[videoIndex];
            document.getElementById('ground-truth-text').textContent = data.groundTruth;
            document.getElementById('model1-text').textContent = data.model1.text;
            document.getElementById('model2-text').textContent = data.model2.text;
            document.getElementById('model3-text').textContent = data.model3.text;
            document.getElementById('model4-text').textContent = data.model4.text;
            document.getElementById('model5-text').textContent = data.model5.text;
            document.getElementById('model6-text').textContent = data.model5.text
                ;

            // Update WER badges
            const werBadges = document.querySelectorAll('.wer-badge');
            werBadges[0].textContent = `WER: ${data.model1.wer}`;
            werBadges[1].textContent = `WER: ${data.model2.wer}`;
            werBadges[2].textContent = `WER: ${data.model3.wer}`;
            werBadges[3].textContent = `WER: ${data.model4.wer}`;
            werBadges[4].textContent = `WER: ${data.model5.wer}`;
            werBadges[5].textContent = `WER: ${data.model6.wer}`;
        }

    </script>


</body>
</html>